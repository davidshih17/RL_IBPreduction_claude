======================================================================
IBP Action Classifier v3 Training
======================================================================
Config:
  data: /home/shih/work/IBPreduction/data/classifier_training_data_p1009_filtered.jsonl
  output_dir: /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered
  epochs: 30
  batch_size: 32
  lr: 0.0001
  weight_decay: 1e-05
  embed_dim: 256
  n_heads: 4
  n_expr_layers: 2
  n_cross_layers: 2
  val_split: 0.1
  seed: 42
  device: cuda
  max_samples: None
  resume: None
  prime: 1009

Loading data...
Loaded 24589 samples from /home/shih/work/IBPreduction/data/classifier_training_data_p1009_filtered.jsonl
Train: 22131, Val: 2458
Train batches: 692, Val batches: 77
IBP environment using PRIME = 1009

Loading IBP environment for end-to-end evaluation...
Loaded 8 IBP templates, 1 LI templates

Creating model (embed_dim=256, n_heads=4, n_expr_layers=2, n_cross_layers=2, prime=1009)...
Model parameters: 6,060,801

Starting training...
======================================================================
  Epoch 1 batch 50/692: loss=1.2892, top1=0.438
  Epoch 1 batch 100/692: loss=1.2815, top1=0.469
  Epoch 1 batch 150/692: loss=1.3993, top1=0.469
  Epoch 1 batch 200/692: loss=2.0350, top1=0.312
  Epoch 1 batch 250/692: loss=1.4147, top1=0.531
  Epoch 1 batch 300/692: loss=1.3510, top1=0.500
  Epoch 1 batch 350/692: loss=1.1947, top1=0.594
  Epoch 1 batch 400/692: loss=1.0541, top1=0.562
  Epoch 1 batch 450/692: loss=0.9810, top1=0.656
  Epoch 1 batch 500/692: loss=0.9250, top1=0.625
  Epoch 1 batch 550/692: loss=1.1626, top1=0.625
  Epoch 1 batch 600/692: loss=0.9409, top1=0.625
  Epoch 1 batch 650/692: loss=0.7392, top1=0.656
/home/shih/.local/lib/python3.12/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
  output = torch._nested_tensor_from_mask(
Epoch 1/30 (77.1s):
  Train: loss=1.3638, top1=0.4811, top5=0.9222
  Val:   loss=0.9699, top1=0.6326, top5=0.9736
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Testing on I[2,0,2,0,1,1,0]...
    Step 0: target=[2, 0, 2, 0, 1, 1, 0] w=(6,0) | 52 actions | chose ibp_op=0 delta=[0, 0, 0, 0, 0, 0, 0] | 2 non-masters left
    Step 1: target=[2, -1, 3, 0, 1, 1, 0] w=(7,1) | 100 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 2: target=[3, -1, 2, 0, 1, 1, 0] w=(7,1) | 140 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 3: target=[3, 0, 1, 0, 1, 1, 0] w=(6,0) | 164 actions | chose ibp_op=1 delta=[0, 0, 0, 0, 0, 0, 0] | 4 non-masters left
    Step 4: target=[3, -1, 1, 0, 1, 2, 0] w=(7,1) | 206 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 2 non-masters left
    Step 5: target=[3, -1, 1, 0, 2, 1, 0] w=(7,1) | 242 actions | chose ibp_op=5 delta=[0, 1, 0, 0, -1, 0, 0] | 1 non-masters left
    Step 6: target=[4, -1, 1, 0, 1, 1, 0] w=(7,1) | 267 actions | chose ibp_op=0 delta=[0, 1, 0, -1, 0, 0, 0] | 4 non-masters left
    Step 7: target=[4, -1, 2, -1, 1, 1, 0] w=(8,2) | 316 actions | chose ibp_op=0 delta=[0, 0, -1, 1, 0, 0, 0] | 1 non-masters left
    Step 8: target=[4, -2, 2, 0, 1, 1, 0] w=(8,2) | 362 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 9: target=[5, -2, 1, 0, 1, 1, 0] w=(8,2) | 389 actions | chose ibp_op=0 delta=[-3, 1, 2, 0, 0, 0, 0] | 3 non-masters left
    Step 10: target=[2, -2, 4, 0, 1, 1, 0] w=(8,2) | 435 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 11: target=[3, -2, 3, 0, 1, 1, 0] w=(8,2) | 462 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 12: target=[1, 0, 3, 0, 1, 1, 0] w=(6,0) | 499 actions | chose ibp_op=2 delta=[0, 0, -1, 0, 0, 0, 0] | 4 non-masters left
    Step 13: target=[1, 0, 3, -1, 1, 1, 0] w=(6,1) | 543 actions | chose ibp_op=3 delta=[0, 0, -1, 1, 0, 0, 0] | 5 non-masters left
    Step 14: target=[1, -1, 2, 0, 1, 2, 0] w=(6,1) | 612 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 5 non-masters left
    Step 15: target=[1, -1, 2, 0, 2, 1, 0] w=(6,1) | 614 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 5 non-masters left
    Step 16: target=[2, 0, 2, -1, 1, 1, 0] w=(6,1) | 545 actions | chose ibp_op=2 delta=[0, 0, -1, 1, 0, 0, 0] | 8 non-masters left
    Step 17: target=[2, -1, 1, 0, 1, 2, 0] w=(6,1) | 680 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 9 non-masters left
    Step 18: target=[2, -1, 1, 0, 2, 1, 0] w=(6,1) | 747 actions | chose ibp_op=5 delta=[0, 1, 0, 0, 0, 0, 0] | 4 non-masters left
    Step 19: target=[2, -1, 2, 0, 2, 1, 0] w=(7,1) | 789 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 20: target=[3, 0, 1, 0, 1, 2, 0] w=(7,0) | 757 actions | chose ibp_op=0 delta=[-1, -2, 3, 0, 0, -1, 0] | 3 non-masters left
    Step 21: target=[2, -3, 5, 0, 1, 1, 0] w=(9,3) | 804 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 22: target=[3, -3, 4, 0, 1, 1, 0] w=(9,3) | 841 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 23: target=[1, -1, 4, 0, 1, 1, 0] w=(7,1) | 873 actions | chose ibp_op=0 delta=[0, 1, -1, -2, 0, 0, 0] | 6 non-masters left
    Step 24: target=[1, -1, 4, -2, 1, 1, 0] w=(7,3) | 932 actions | chose ibp_op=0 delta=[0, 1, -1, 1, 0, 0, 0] | 5 non-masters left
    Step 25: target=[1, -1, 4, -1, 1, 1, 0] w=(7,2) | 960 actions | chose ibp_op=2 delta=[0, 0, -1, 1, 0, 0, 0] | 6 non-masters left
    Step 26: target=[1, -2, 4, 0, 1, 1, 0] w=(7,2) | 999 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 4 non-masters left
    Step 27: target=[2, -1, 3, -1, 1, 1, 0] w=(7,2) | 987 actions | chose ibp_op=2 delta=[0, 0, -1, 1, 0, 0, 0] | 6 non-masters left
    Step 28: target=[2, -2, 3, 0, 1, 1, 0] w=(7,2) | 1028 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 3 non-masters left
    Step 29: target=[3, -1, 2, -1, 1, 1, 0] w=(7,2) | 1012 actions | chose ibp_op=0 delta=[0, 1, -1, 1, 0, 1, 0] | 3 non-masters left
    Step 30: target=[3, -1, 2, 0, 1, 2, 0] w=(8,1) | 1061 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 5 non-masters left
    Step 31: target=[3, -1, 2, 0, 2, 1, 0] w=(8,1) | 1114 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 6 non-masters left
    Step 32: target=[3, 0, 2, 0, 1, 2, 0] w=(8,0) | 1111 actions | chose ibp_op=0 delta=[2, -2, -1, 0, 0, -1, 0] | 1 non-masters left
    Step 33: target=[5, -3, 2, 0, 1, 1, 0] w=(9,3) | 1157 actions | chose ibp_op=0 delta=[-1, 1, 0, 0, 0, 0, 0] | 1 non-masters left
    Step 34: target=[4, -3, 3, 0, 1, 1, 0] w=(9,3) | 1180 actions | chose ibp_op=0 delta=[0, 2, -1, -2, 0, 0, 0] | 8 non-masters left
    Step 35: target=[4, -2, 3, -2, 1, 1, 0] w=(9,4) | 1240 actions | chose ibp_op=0 delta=[0, 1, -1, 1, 0, 0, 0] | 6 non-masters left
    Step 36: target=[4, -2, 3, -1, 1, 1, 0] w=(9,3) | 1270 actions | chose ibp_op=2 delta=[0, 0, -1, 1, 0, 0, 0] | 7 non-masters left
    Step 37: target=[5, -2, 2, -1, 1, 1, 0] w=(9,3) | 1307 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 8 non-masters left
    Step 38: target=[5, -2, 2, 0, 1, 1, 0] w=(9,2) | 1351 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 7 non-masters left
    Step 39: target=[3, -1, 3, -1, 1, 1, 0] w=(8,2) | 1338 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 8 non-masters left
    Step 40: target=[5, -1, 1, -1, 1, 1, 0] w=(8,2) | 1362 actions | chose ibp_op=0 delta=[-3, -2, 4, 1, 0, 0, 0] | 3 non-masters left
    Step 41: target=[2, -4, 6, 0, 1, 1, 0] w=(10,4) | 1409 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 42: target=[3, -4, 5, 0, 1, 1, 0] w=(10,4) | 1447 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 43: target=[1, -2, 5, 0, 1, 1, 0] w=(8,2) | 1479 actions | chose ibp_op=0 delta=[0, 1, -1, -3, 0, 0, 0] | 7 non-masters left
    Step 44: target=[1, -2, 5, -3, 1, 1, 0] w=(8,5) | 1538 actions | chose ibp_op=0 delta=[0, 1, -1, 2, 0, 0, 0] | 5 non-masters left
    Step 45: target=[1, -2, 5, -1, 1, 1, 0] w=(8,3) | 1575 actions | chose ibp_op=2 delta=[0, 0, -1, 1, 0, 0, 0] | 6 non-masters left
    Step 46: target=[1, -3, 5, 0, 1, 1, 0] w=(8,3) | 1615 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 5 non-masters left
    Step 47: target=[2, -2, 4, -1, 1, 1, 0] w=(8,3) | 1606 actions | chose ibp_op=2 delta=[0, 0, -1, 1, 0, 0, 0] | 6 non-masters left
    Step 48: target=[2, -3, 4, 0, 1, 1, 0] w=(8,3) | 1648 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 4 non-masters left
    Step 49: target=[3, -2, 3, -1, 1, 1, 0] w=(8,3) | 1626 actions | chose ibp_op=2 delta=[0, 0, -1, 1, 0, 0, 0] | 7 non-masters left
  I[2,0,2,0,1,1,0]: FAILED (7 non-masters) in 50 steps
  Epoch 2 batch 50/692: loss=1.3059, top1=0.500
  Epoch 2 batch 100/692: loss=1.0172, top1=0.562
  Epoch 2 batch 150/692: loss=0.8391, top1=0.625
  Epoch 2 batch 200/692: loss=1.0262, top1=0.625
  Epoch 2 batch 250/692: loss=0.9773, top1=0.562
  Epoch 2 batch 300/692: loss=0.4884, top1=0.844
  Epoch 2 batch 350/692: loss=0.9921, top1=0.719
  Epoch 2 batch 400/692: loss=0.9399, top1=0.719
  Epoch 2 batch 450/692: loss=1.1299, top1=0.656
  Epoch 2 batch 500/692: loss=0.7639, top1=0.625
  Epoch 2 batch 550/692: loss=0.6871, top1=0.688
  Epoch 2 batch 600/692: loss=0.9055, top1=0.719
  Epoch 2 batch 650/692: loss=0.7360, top1=0.750
Epoch 2/30 (76.5s):
  Train: loss=0.8723, top1=0.6791, top5=0.9775
  Val:   loss=0.6658, top1=0.7579, top5=0.9874
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 3 batch 50/692: loss=0.5432, top1=0.812
  Epoch 3 batch 100/692: loss=0.8866, top1=0.688
  Epoch 3 batch 150/692: loss=0.4556, top1=0.844
  Epoch 3 batch 200/692: loss=0.4879, top1=0.844
  Epoch 3 batch 250/692: loss=0.5065, top1=0.781
  Epoch 3 batch 300/692: loss=0.5901, top1=0.781
  Epoch 3 batch 350/692: loss=0.6666, top1=0.750
  Epoch 3 batch 400/692: loss=0.9277, top1=0.656
  Epoch 3 batch 450/692: loss=0.6391, top1=0.750
  Epoch 3 batch 500/692: loss=0.6740, top1=0.719
  Epoch 3 batch 550/692: loss=0.3880, top1=0.812
  Epoch 3 batch 600/692: loss=0.4330, top1=0.812
  Epoch 3 batch 650/692: loss=0.7028, top1=0.875
Epoch 3/30 (76.2s):
  Train: loss=0.6766, top1=0.7639, top5=0.9846
  Val:   loss=0.5497, top1=0.8116, top5=0.9886
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 4 batch 50/692: loss=0.4989, top1=0.844
  Epoch 4 batch 100/692: loss=0.5913, top1=0.875
  Epoch 4 batch 150/692: loss=0.3994, top1=0.844
  Epoch 4 batch 200/692: loss=0.7822, top1=0.812
  Epoch 4 batch 250/692: loss=0.4193, top1=0.875
  Epoch 4 batch 300/692: loss=0.8862, top1=0.594
  Epoch 4 batch 350/692: loss=0.5574, top1=0.812
  Epoch 4 batch 400/692: loss=0.7135, top1=0.781
  Epoch 4 batch 450/692: loss=0.3080, top1=0.938
  Epoch 4 batch 500/692: loss=0.6448, top1=0.781
  Epoch 4 batch 550/692: loss=0.2747, top1=0.906
  Epoch 4 batch 600/692: loss=0.7844, top1=0.656
  Epoch 4 batch 650/692: loss=0.5420, top1=0.875
Epoch 4/30 (79.4s):
  Train: loss=0.5524, top1=0.8154, top5=0.9890
  Val:   loss=0.4759, top1=0.8324, top5=0.9882
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 5 batch 50/692: loss=0.7063, top1=0.781
  Epoch 5 batch 100/692: loss=0.5538, top1=0.875
  Epoch 5 batch 150/692: loss=0.4469, top1=0.875
  Epoch 5 batch 200/692: loss=0.5623, top1=0.844
  Epoch 5 batch 250/692: loss=0.5168, top1=0.719
  Epoch 5 batch 300/692: loss=0.4778, top1=0.844
  Epoch 5 batch 350/692: loss=0.7311, top1=0.781
  Epoch 5 batch 400/692: loss=0.7484, top1=0.750
  Epoch 5 batch 450/692: loss=0.4948, top1=0.781
  Epoch 5 batch 500/692: loss=0.4853, top1=0.906
  Epoch 5 batch 550/692: loss=0.3506, top1=0.938
  Epoch 5 batch 600/692: loss=0.4316, top1=0.750
  Epoch 5 batch 650/692: loss=0.6071, top1=0.719
Epoch 5/30 (77.0s):
  Train: loss=0.4789, top1=0.8446, top5=0.9902
  Val:   loss=0.4384, top1=0.8576, top5=0.9894
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 6 batch 50/692: loss=0.3436, top1=0.906
  Epoch 6 batch 100/692: loss=0.3643, top1=0.938
  Epoch 6 batch 150/692: loss=0.4081, top1=0.875
  Epoch 6 batch 200/692: loss=0.3757, top1=0.875
  Epoch 6 batch 250/692: loss=0.4614, top1=0.875
  Epoch 6 batch 300/692: loss=0.4480, top1=0.844
  Epoch 6 batch 350/692: loss=0.2360, top1=0.938
  Epoch 6 batch 400/692: loss=0.3595, top1=0.906
  Epoch 6 batch 450/692: loss=0.2802, top1=0.906
  Epoch 6 batch 500/692: loss=0.4954, top1=0.875
  Epoch 6 batch 550/692: loss=0.2855, top1=0.844
  Epoch 6 batch 600/692: loss=0.4306, top1=0.844
  Epoch 6 batch 650/692: loss=0.6205, top1=0.750
Epoch 6/30 (76.5s):
  Train: loss=0.4193, top1=0.8663, top5=0.9920
  Val:   loss=0.4088, top1=0.8702, top5=0.9919
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 7 batch 50/692: loss=0.4857, top1=0.969
  Epoch 7 batch 100/692: loss=0.2060, top1=0.938
  Epoch 7 batch 150/692: loss=0.4926, top1=0.781
  Epoch 7 batch 200/692: loss=0.6236, top1=0.844
  Epoch 7 batch 250/692: loss=0.4976, top1=0.875
  Epoch 7 batch 300/692: loss=0.6726, top1=0.812
  Epoch 7 batch 350/692: loss=0.1677, top1=0.969
  Epoch 7 batch 400/692: loss=0.3726, top1=0.844
  Epoch 7 batch 450/692: loss=0.7646, top1=0.750
  Epoch 7 batch 500/692: loss=0.5207, top1=0.812
  Epoch 7 batch 550/692: loss=0.4910, top1=0.844
  Epoch 7 batch 600/692: loss=0.4223, top1=0.875
  Epoch 7 batch 650/692: loss=0.5591, top1=0.844
Epoch 7/30 (76.6s):
  Train: loss=0.3817, top1=0.8813, top5=0.9930
  Val:   loss=0.3737, top1=0.8796, top5=0.9923
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 8 batch 50/692: loss=0.3460, top1=0.875
  Epoch 8 batch 100/692: loss=0.2402, top1=0.906
  Epoch 8 batch 150/692: loss=0.4820, top1=0.906
  Epoch 8 batch 200/692: loss=0.3205, top1=0.938
  Epoch 8 batch 250/692: loss=0.1366, top1=0.938
  Epoch 8 batch 300/692: loss=0.1850, top1=0.906
  Epoch 8 batch 350/692: loss=0.6963, top1=0.781
  Epoch 8 batch 400/692: loss=1.0592, top1=0.750
  Epoch 8 batch 450/692: loss=0.2974, top1=0.875
  Epoch 8 batch 500/692: loss=0.1140, top1=0.938
  Epoch 8 batch 550/692: loss=0.4347, top1=0.812
  Epoch 8 batch 600/692: loss=0.3197, top1=0.906
  Epoch 8 batch 650/692: loss=0.4462, top1=0.844
Epoch 8/30 (76.8s):
  Train: loss=0.3478, top1=0.8921, top5=0.9926
  Val:   loss=0.3111, top1=0.8946, top5=0.9923
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 9 batch 50/692: loss=0.1683, top1=0.938
  Epoch 9 batch 100/692: loss=0.3766, top1=0.906
  Epoch 9 batch 150/692: loss=0.2671, top1=0.906
  Epoch 9 batch 200/692: loss=0.2518, top1=0.906
  Epoch 9 batch 250/692: loss=0.0622, top1=1.000
  Epoch 9 batch 300/692: loss=0.2409, top1=0.875
  Epoch 9 batch 350/692: loss=0.3924, top1=0.844
  Epoch 9 batch 400/692: loss=0.2925, top1=0.906
  Epoch 9 batch 450/692: loss=0.6670, top1=0.844
  Epoch 9 batch 500/692: loss=0.1516, top1=0.969
  Epoch 9 batch 550/692: loss=0.3869, top1=0.844
  Epoch 9 batch 600/692: loss=0.5403, top1=0.844
  Epoch 9 batch 650/692: loss=0.1127, top1=0.969
Epoch 9/30 (76.2s):
  Train: loss=0.3111, top1=0.9030, top5=0.9943
  Val:   loss=0.3070, top1=0.9097, top5=0.9959
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 10 batch 50/692: loss=0.2955, top1=0.906
  Epoch 10 batch 100/692: loss=0.3680, top1=0.844
  Epoch 10 batch 150/692: loss=0.1292, top1=0.969
  Epoch 10 batch 200/692: loss=0.4525, top1=0.938
  Epoch 10 batch 250/692: loss=0.0923, top1=0.969
  Epoch 10 batch 300/692: loss=0.6521, top1=0.812
  Epoch 10 batch 350/692: loss=0.2703, top1=0.938
  Epoch 10 batch 400/692: loss=0.6325, top1=0.844
  Epoch 10 batch 450/692: loss=0.1724, top1=0.938
  Epoch 10 batch 500/692: loss=0.2801, top1=0.938
  Epoch 10 batch 550/692: loss=0.3088, top1=0.938
  Epoch 10 batch 600/692: loss=0.1803, top1=0.938
  Epoch 10 batch 650/692: loss=0.1933, top1=0.938
Epoch 10/30 (76.3s):
  Train: loss=0.2902, top1=0.9115, top5=0.9957
  Val:   loss=0.2766, top1=0.9125, top5=0.9935
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Testing on I[2,0,2,0,1,1,0]...
    Step 0: target=[2, 0, 2, 0, 1, 1, 0] w=(6,0) | 52 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 0, 0, 0] | 8 non-masters left
    Step 1: target=[2, 0, 2, 0, 1, 1, -1] w=(6,1) | 100 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 1, 0, 1] | 10 non-masters left
    Step 2: target=[2, 0, 2, 0, 2, 1, -1] w=(7,1) | 154 actions | chose ibp_op=7 delta=[0, 0, -1, 0, -1, 1, 1] | 10 non-masters left
    Step 3: target=[2, 0, 2, 0, 1, 2, -1] w=(7,1) | 200 actions | chose ibp_op=7 delta=[0, 0, 0, 0, 0, -1, 1] | 8 non-masters left
    Step 4: target=[2, 0, 3, 0, 1, 1, -1] w=(7,1) | 238 actions | chose ibp_op=1 delta=[0, 0, -1, 0, 0, 0, 1] | 6 non-masters left
    Step 5: target=[2, -1, 2, 0, 1, 2, 0] w=(7,1) | 301 actions | chose ibp_op=0 delta=[0, 1, 0, 0, 0, -1, 0] | 2 non-masters left
    Step 6: target=[2, -1, 3, 0, 1, 1, 0] w=(7,1) | 345 actions | chose ibp_op=5 delta=[-1, 1, 0, 0, 0, 0, 0] | 2 non-masters left
    Step 7: target=[1, -1, 4, 0, 1, 1, 0] w=(7,1) | 389 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 8: target=[1, 0, 3, 0, 1, 1, 0] w=(6,0) | 406 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 0, 0, 0] | 8 non-masters left
    Step 9: target=[1, 0, 3, 0, 1, 1, -1] w=(6,1) | 433 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 1, 0, 1] | 10 non-masters left
    Step 10: target=[1, 0, 3, 0, 2, 1, -1] w=(7,1) | 474 actions | chose ibp_op=7 delta=[0, 0, -1, 0, -1, 1, 1] | 10 non-masters left
    Step 11: target=[1, 0, 3, 0, 1, 2, -1] w=(7,1) | 508 actions | chose ibp_op=7 delta=[0, 0, 0, 0, 0, -1, 1] | 7 non-masters left
    Step 12: target=[1, 0, 4, 0, 1, 1, -1] w=(7,1) | 538 actions | chose ibp_op=1 delta=[0, 0, -1, 0, 0, 0, 1] | 4 non-masters left
    Step 13: target=[1, -1, 3, 0, 1, 2, 0] w=(7,1) | 582 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 2 non-masters left
    Step 14: target=[1, -1, 3, 0, 2, 1, 0] w=(7,1) | 631 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 15: target=[1, 0, 3, 0, 1, 2, 0] w=(7,0) | 623 actions | chose ibp_op=0 delta=[0, 0, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 16: target=[1, 0, 3, 0, 2, 1, 0] w=(7,0) | 616 actions | chose ibp_op=0 delta=[0, 0, 0, 0, 0, 0, 0] | 2 non-masters left
    Step 17: target=[1, -1, 4, 0, 2, 1, 0] w=(8,1) | 668 actions | chose ibp_op=0 delta=[0, 1, -1, 0, -1, 1, 0] | 2 non-masters left
    Step 18: target=[1, -1, 4, 0, 1, 2, 0] w=(8,1) | 703 actions | chose ibp_op=0 delta=[2, 1, -3, 0, 0, -1, 0] | 2 non-masters left
    Step 19: target=[3, -1, 2, 0, 1, 1, 0] w=(7,1) | 738 actions | chose ibp_op=5 delta=[-1, 1, 0, 0, 0, 0, 0] | 1 non-masters left
    Step 20: target=[3, 0, 1, 0, 1, 1, 0] w=(6,0) | 758 actions | chose ibp_op=1 delta=[0, 0, 0, 0, 0, 0, 0] | 4 non-masters left
    Step 21: target=[3, -1, 1, 0, 1, 2, 0] w=(7,1) | 791 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 2 non-masters left
    Step 22: target=[3, -1, 1, 0, 2, 1, 0] w=(7,1) | 827 actions | chose ibp_op=5 delta=[0, 1, 0, 0, -1, 0, 0] | 1 non-masters left
    Step 23: target=[4, -1, 1, 0, 1, 1, 0] w=(7,1) | 852 actions | chose ibp_op=8 delta=[-3, 1, 3, 0, 0, 0, 0] | 4 non-masters left
    Step 24: target=[1, 0, 4, 0, 1, 2, 0] w=(8,0) | 898 actions | chose ibp_op=3 delta=[0, 0, -1, 0, 0, -1, 0] | 2 non-masters left
    Step 25: target=[1, 0, 4, -1, 1, 1, 0] w=(7,1) | 928 actions | chose ibp_op=0 delta=[0, 0, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 26: target=[1, -1, 4, -1, 1, 1, 0] w=(7,2) | 969 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 1, 0] | 3 non-masters left
    Step 27: target=[1, -1, 4, -1, 1, 2, 0] w=(8,2) | 1009 actions | chose ibp_op=0 delta=[3, 1, -3, 0, 0, -1, 0] | 4 non-masters left
    Step 28: target=[4, -1, 2, -1, 1, 1, 0] w=(8,2) | 1059 actions | chose ibp_op=0 delta=[-1, 1, 0, 0, 0, 0, 0] | 4 non-masters left
    Step 29: target=[3, -1, 3, -1, 1, 1, 0] w=(8,2) | 1101 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 3 non-masters left
    Step 30: target=[3, 0, 2, -1, 1, 1, 0] w=(7,1) | 1125 actions | chose ibp_op=0 delta=[0, 0, -1, 0, 0, 0, 0] | 4 non-masters left
    Step 31: target=[3, -1, 2, -1, 1, 1, 0] w=(7,2) | 1156 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 1, 0] | 6 non-masters left
    Step 32: target=[3, -1, 2, -1, 1, 2, 0] w=(8,2) | 1194 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 33: target=[4, -1, 1, -1, 1, 2, 0] w=(8,2) | 1187 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 7 non-masters left
    Step 34: target=[4, -1, 1, -1, 2, 1, 0] w=(8,2) | 1219 actions | chose ibp_op=0 delta=[-2, 1, 1, 1, 0, 0, 0] | 3 non-masters left
    Step 35: target=[2, -1, 3, 0, 2, 1, 0] w=(8,1) | 1262 actions | chose ibp_op=0 delta=[0, 1, -1, 0, -1, 1, 0] | 3 non-masters left
    Step 36: target=[2, -1, 3, 0, 1, 2, 0] w=(8,1) | 1286 actions | chose ibp_op=0 delta=[0, 1, 0, 0, 0, 0, 0] | 5 non-masters left
    Step 37: target=[2, -1, 4, 0, 1, 2, 0] w=(9,1) | 1337 actions | chose ibp_op=0 delta=[0, 0, -1, 0, 0, -1, 0] | 1 non-masters left
    Step 38: target=[2, -2, 4, 0, 1, 1, 0] w=(8,2) | 1384 actions | chose ibp_op=0 delta=[-1, 1, 0, 0, 0, 0, 0] | 1 non-masters left
    Step 39: target=[1, -2, 5, 0, 1, 1, 0] w=(8,2) | 1416 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 1, 0] | 2 non-masters left
    Step 40: target=[1, -2, 5, 0, 1, 2, 0] w=(9,2) | 1463 actions | chose ibp_op=1 delta=[0, 1, 0, 0, 0, -1, 0] | 6 non-masters left
    Step 41: target=[1, -2, 5, 0, 2, 1, 0] w=(9,2) | 1497 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 6 non-masters left
    Step 42: target=[1, -2, 6, 0, 1, 1, 0] w=(9,2) | 1542 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 5 non-masters left
    Step 43: target=[1, -1, 5, 0, 1, 2, 0] w=(9,1) | 1538 actions | chose ibp_op=1 delta=[0, 1, 0, 0, 0, -1, 0] | 8 non-masters left
    Step 44: target=[1, -1, 5, 0, 2, 1, 0] w=(9,1) | 1580 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 8 non-masters left
    Step 45: target=[1, -1, 6, 0, 1, 1, 0] w=(9,1) | 1580 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 7 non-masters left
    Step 46: target=[1, 0, 5, 0, 1, 2, 0] w=(9,0) | 1584 actions | chose ibp_op=3 delta=[0, 0, -1, 0, 0, -1, 0] | 5 non-masters left
    Step 47: target=[1, 0, 5, -1, 1, 1, 0] w=(8,1) | 1634 actions | chose ibp_op=1 delta=[0, 0, -1, 1, 0, 0, 0] | 4 non-masters left
    Step 48: target=[1, -1, 5, 0, 1, 1, 0] w=(8,1) | 1651 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 3 non-masters left
    Step 49: target=[1, 0, 4, 0, 2, 1, 0] w=(8,0) | 1708 actions | chose ibp_op=5 delta=[0, 0, -1, 0, 0, 0, 0] | 2 non-masters left
  I[2,0,2,0,1,1,0]: FAILED (2 non-masters) in 50 steps
  Saved checkpoint to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/checkpoint_epoch10.pt
  Epoch 11 batch 50/692: loss=0.1534, top1=0.906
  Epoch 11 batch 100/692: loss=0.2107, top1=0.906
  Epoch 11 batch 150/692: loss=0.1976, top1=0.906
  Epoch 11 batch 200/692: loss=0.3435, top1=0.875
  Epoch 11 batch 250/692: loss=0.1074, top1=0.969
  Epoch 11 batch 300/692: loss=0.0945, top1=0.938
  Epoch 11 batch 350/692: loss=0.2208, top1=0.906
  Epoch 11 batch 400/692: loss=0.0580, top1=0.969
  Epoch 11 batch 450/692: loss=0.1350, top1=0.938
  Epoch 11 batch 500/692: loss=0.3842, top1=0.906
  Epoch 11 batch 550/692: loss=0.1084, top1=0.969
  Epoch 11 batch 600/692: loss=0.2172, top1=0.938
  Epoch 11 batch 650/692: loss=0.0457, top1=1.000
Epoch 11/30 (75.9s):
  Train: loss=0.2678, top1=0.9181, top5=0.9953
  Val:   loss=0.2712, top1=0.9223, top5=0.9935
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 12 batch 50/692: loss=0.1351, top1=0.938
  Epoch 12 batch 100/692: loss=0.0355, top1=1.000
  Epoch 12 batch 150/692: loss=0.1392, top1=0.969
  Epoch 12 batch 200/692: loss=0.1263, top1=0.938
  Epoch 12 batch 250/692: loss=0.7166, top1=0.812
  Epoch 12 batch 300/692: loss=0.3887, top1=0.906
  Epoch 12 batch 350/692: loss=0.1881, top1=0.906
  Epoch 12 batch 400/692: loss=0.3143, top1=0.938
  Epoch 12 batch 450/692: loss=0.1702, top1=0.938
  Epoch 12 batch 500/692: loss=0.3674, top1=0.844
  Epoch 12 batch 550/692: loss=0.4181, top1=0.812
  Epoch 12 batch 600/692: loss=0.1151, top1=0.969
  Epoch 12 batch 650/692: loss=0.1452, top1=0.938
Epoch 12/30 (76.2s):
  Train: loss=0.2472, top1=0.9235, top5=0.9961
  Val:   loss=0.2463, top1=0.9276, top5=0.9951
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 13 batch 50/692: loss=0.2853, top1=0.938
  Epoch 13 batch 100/692: loss=0.0644, top1=1.000
  Epoch 13 batch 150/692: loss=0.0214, top1=1.000
  Epoch 13 batch 200/692: loss=0.1413, top1=0.938
  Epoch 13 batch 250/692: loss=0.1402, top1=0.969
  Epoch 13 batch 300/692: loss=0.0896, top1=0.938
  Epoch 13 batch 350/692: loss=0.2194, top1=0.906
  Epoch 13 batch 400/692: loss=0.3474, top1=0.906
  Epoch 13 batch 450/692: loss=0.1549, top1=0.969
  Epoch 13 batch 500/692: loss=0.1795, top1=0.969
  Epoch 13 batch 550/692: loss=0.1070, top1=0.969
  Epoch 13 batch 600/692: loss=0.1202, top1=0.969
  Epoch 13 batch 650/692: loss=0.3144, top1=0.906
Epoch 13/30 (75.8s):
  Train: loss=0.2317, top1=0.9305, top5=0.9965
  Val:   loss=0.2266, top1=0.9353, top5=0.9959
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 14 batch 50/692: loss=0.2685, top1=0.906
  Epoch 14 batch 100/692: loss=0.4116, top1=0.812
  Epoch 14 batch 150/692: loss=0.6444, top1=0.875
  Epoch 14 batch 200/692: loss=0.1138, top1=0.938
  Epoch 14 batch 250/692: loss=0.0944, top1=0.969
  Epoch 14 batch 300/692: loss=0.1384, top1=0.969
  Epoch 14 batch 350/692: loss=0.0907, top1=0.969
  Epoch 14 batch 400/692: loss=0.1475, top1=0.875
  Epoch 14 batch 450/692: loss=0.2702, top1=0.906
  Epoch 14 batch 500/692: loss=0.1987, top1=0.938
  Epoch 14 batch 550/692: loss=0.0247, top1=1.000
  Epoch 14 batch 600/692: loss=0.6071, top1=0.812
  Epoch 14 batch 650/692: loss=0.1548, top1=0.938
Epoch 14/30 (78.3s):
  Train: loss=0.2146, top1=0.9370, top5=0.9967
  Val:   loss=0.2352, top1=0.9329, top5=0.9963
  Epoch 15 batch 50/692: loss=0.0957, top1=0.969
  Epoch 15 batch 100/692: loss=0.1271, top1=0.969
  Epoch 15 batch 150/692: loss=0.1450, top1=0.906
  Epoch 15 batch 200/692: loss=0.3985, top1=0.875
  Epoch 15 batch 250/692: loss=0.2824, top1=0.875
  Epoch 15 batch 300/692: loss=0.1027, top1=0.969
  Epoch 15 batch 350/692: loss=0.1599, top1=0.938
  Epoch 15 batch 400/692: loss=0.0473, top1=0.969
  Epoch 15 batch 450/692: loss=0.1190, top1=0.969
  Epoch 15 batch 500/692: loss=0.1477, top1=0.969
  Epoch 15 batch 550/692: loss=0.0386, top1=0.969
  Epoch 15 batch 600/692: loss=0.5832, top1=0.938
  Epoch 15 batch 650/692: loss=0.1212, top1=0.969
Epoch 15/30 (82.2s):
  Train: loss=0.2005, top1=0.9388, top5=0.9973
  Val:   loss=0.2112, top1=0.9394, top5=0.9955
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 16 batch 50/692: loss=0.3449, top1=0.906
  Epoch 16 batch 100/692: loss=0.1871, top1=0.969
  Epoch 16 batch 150/692: loss=0.1297, top1=0.969
  Epoch 16 batch 200/692: loss=0.0112, top1=1.000
  Epoch 16 batch 250/692: loss=0.3732, top1=0.938
  Epoch 16 batch 300/692: loss=0.0793, top1=0.969
  Epoch 16 batch 350/692: loss=0.4876, top1=0.875
  Epoch 16 batch 400/692: loss=0.2482, top1=0.938
  Epoch 16 batch 450/692: loss=0.0581, top1=0.969
  Epoch 16 batch 500/692: loss=0.1997, top1=0.906
  Epoch 16 batch 550/692: loss=0.2525, top1=0.906
  Epoch 16 batch 600/692: loss=0.0772, top1=0.969
  Epoch 16 batch 650/692: loss=0.2098, top1=0.969
Epoch 16/30 (78.6s):
  Train: loss=0.1896, top1=0.9442, top5=0.9968
  Val:   loss=0.2225, top1=0.9402, top5=0.9963
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 17 batch 50/692: loss=0.0708, top1=0.969
  Epoch 17 batch 100/692: loss=0.0257, top1=1.000
  Epoch 17 batch 150/692: loss=0.0404, top1=1.000
  Epoch 17 batch 200/692: loss=0.2172, top1=0.938
  Epoch 17 batch 250/692: loss=0.2474, top1=0.938
  Epoch 17 batch 300/692: loss=0.1108, top1=0.938
  Epoch 17 batch 350/692: loss=0.2184, top1=0.938
  Epoch 17 batch 400/692: loss=0.2503, top1=0.875
  Epoch 17 batch 450/692: loss=0.3554, top1=0.938
  Epoch 17 batch 500/692: loss=0.2600, top1=0.906
  Epoch 17 batch 550/692: loss=0.0603, top1=0.969
  Epoch 17 batch 600/692: loss=0.0880, top1=0.969
  Epoch 17 batch 650/692: loss=0.1377, top1=0.969
Epoch 17/30 (76.5s):
  Train: loss=0.1737, top1=0.9494, top5=0.9977
  Val:   loss=0.2150, top1=0.9455, top5=0.9959
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 18 batch 50/692: loss=0.0698, top1=1.000
  Epoch 18 batch 100/692: loss=0.0791, top1=0.969
  Epoch 18 batch 150/692: loss=0.0910, top1=0.969
  Epoch 18 batch 200/692: loss=0.2572, top1=0.906
  Epoch 18 batch 250/692: loss=0.0278, top1=1.000
  Epoch 18 batch 300/692: loss=0.1535, top1=0.969
  Epoch 18 batch 350/692: loss=0.2495, top1=0.938
  Epoch 18 batch 400/692: loss=0.2487, top1=0.906
  Epoch 18 batch 450/692: loss=0.0595, top1=0.969
  Epoch 18 batch 500/692: loss=0.0925, top1=0.969
  Epoch 18 batch 550/692: loss=0.1274, top1=0.938
  Epoch 18 batch 600/692: loss=0.6141, top1=0.906
  Epoch 18 batch 650/692: loss=0.1338, top1=0.938
Epoch 18/30 (82.3s):
  Train: loss=0.1612, top1=0.9540, top5=0.9977
  Val:   loss=0.2317, top1=0.9426, top5=0.9959
  Epoch 19 batch 50/692: loss=0.0612, top1=0.969
  Epoch 19 batch 100/692: loss=0.1821, top1=0.938
  Epoch 19 batch 150/692: loss=0.1092, top1=0.969
  Epoch 19 batch 200/692: loss=0.0580, top1=1.000
  Epoch 19 batch 250/692: loss=0.0680, top1=0.969
  Epoch 19 batch 300/692: loss=0.2984, top1=0.938
  Epoch 19 batch 350/692: loss=0.2683, top1=0.906
  Epoch 19 batch 400/692: loss=0.0525, top1=1.000
  Epoch 19 batch 450/692: loss=0.1810, top1=0.938
  Epoch 19 batch 500/692: loss=0.0827, top1=0.969
  Epoch 19 batch 550/692: loss=0.4969, top1=0.875
  Epoch 19 batch 600/692: loss=0.1120, top1=0.969
  Epoch 19 batch 650/692: loss=0.2103, top1=0.938
Epoch 19/30 (79.0s):
  Train: loss=0.1500, top1=0.9556, top5=0.9984
  Val:   loss=0.2059, top1=0.9483, top5=0.9963
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 20 batch 50/692: loss=0.0195, top1=1.000
  Epoch 20 batch 100/692: loss=0.0508, top1=0.969
  Epoch 20 batch 150/692: loss=0.1113, top1=0.938
  Epoch 20 batch 200/692: loss=0.0883, top1=0.938
  Epoch 20 batch 250/692: loss=0.0784, top1=0.969
  Epoch 20 batch 300/692: loss=0.1943, top1=0.906
  Epoch 20 batch 350/692: loss=0.1335, top1=0.938
  Epoch 20 batch 400/692: loss=0.1669, top1=0.969
  Epoch 20 batch 450/692: loss=0.3567, top1=0.906
  Epoch 20 batch 500/692: loss=0.0543, top1=0.969
  Epoch 20 batch 550/692: loss=0.0480, top1=1.000
  Epoch 20 batch 600/692: loss=0.2108, top1=0.938
  Epoch 20 batch 650/692: loss=0.0964, top1=0.969
Epoch 20/30 (79.0s):
  Train: loss=0.1349, top1=0.9605, top5=0.9984
  Val:   loss=0.2277, top1=0.9443, top5=0.9959
  Testing on I[2,0,2,0,1,1,0]...
    Step 0: target=[2, 0, 2, 0, 1, 1, 0] w=(6,0) | 52 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 0, 0, 0] | 8 non-masters left
    Step 1: target=[2, 0, 2, 0, 1, 1, -1] w=(6,1) | 100 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 1, 0, 1] | 10 non-masters left
    Step 2: target=[2, 0, 2, 0, 2, 1, -1] w=(7,1) | 154 actions | chose ibp_op=3 delta=[0, 0, -1, 0, -1, 1, 1] | 9 non-masters left
    Step 3: target=[2, 0, 2, -1, 1, 2, 0] w=(7,1) | 207 actions | chose ibp_op=0 delta=[0, 0, 0, 1, 0, -1, 0] | 2 non-masters left
    Step 4: target=[2, -1, 3, 0, 1, 1, 0] w=(7,1) | 256 actions | chose ibp_op=5 delta=[-1, 1, 0, 0, 0, 0, 0] | 2 non-masters left
    Step 5: target=[1, -1, 4, 0, 1, 1, 0] w=(7,1) | 302 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 6: target=[1, 0, 3, 0, 1, 1, 0] w=(6,0) | 322 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 0, 0, 0] | 8 non-masters left
    Step 7: target=[1, 0, 3, 0, 1, 1, -1] w=(6,1) | 357 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 1, 0, 1] | 10 non-masters left
    Step 8: target=[1, 0, 3, 0, 2, 1, -1] w=(7,1) | 398 actions | chose ibp_op=7 delta=[0, 0, -1, 0, -1, 1, 1] | 10 non-masters left
    Step 9: target=[1, 0, 3, 0, 1, 2, -1] w=(7,1) | 439 actions | chose ibp_op=5 delta=[1, 0, -1, 0, 0, -1, 1] | 2 non-masters left
    Step 10: target=[3, -1, 2, 0, 1, 1, 0] w=(7,1) | 482 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 11: target=[4, -1, 1, 0, 1, 1, 0] w=(7,1) | 510 actions | chose ibp_op=0 delta=[-1, 1, 0, 0, 0, 0, 0] | 1 non-masters left
    Step 12: target=[3, 0, 1, 0, 1, 1, 0] w=(6,0) | 523 actions | chose ibp_op=1 delta=[0, 0, 0, 0, 0, 0, 0] | 4 non-masters left
    Step 13: target=[3, -1, 1, 0, 1, 2, 0] w=(7,1) | 564 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 2 non-masters left
    Step 14: target=[3, -1, 1, 0, 2, 1, 0] w=(7,1) | 599 actions | chose ibp_op=5 delta=[-1, 1, 0, 0, 0, 0, 0] | 4 non-masters left
    Step 15: target=[2, -1, 2, 0, 2, 1, 0] w=(7,1) | 653 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 16: target=[3, 0, 1, 0, 1, 2, 0] w=(7,0) | 600 actions | chose ibp_op=0 delta=[0, 0, 0, 0, 0, 0, 0] | 3 non-masters left
    Step 17: target=[3, -1, 2, 0, 1, 2, 0] w=(8,1) | 649 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 5 non-masters left
    Step 18: target=[3, -1, 2, 0, 2, 1, 0] w=(8,1) | 702 actions | chose ibp_op=8 delta=[0, 1, -1, 0, 0, 0, 0] | 6 non-masters left
    Step 19: target=[3, -1, 1, 0, 2, 2, 0] w=(8,1) | 739 actions | chose ibp_op=6 delta=[0, 1, 0, 0, 0, -1, 0] | 10 non-masters left
    Step 20: target=[3, 0, 2, 0, 2, 1, -1] w=(8,1) | 791 actions | chose ibp_op=6 delta=[0, 0, -1, 0, -1, 0, 1] | 6 non-masters left
    Step 21: target=[3, 0, 2, 0, 1, 1, -1] w=(7,1) | 815 actions | chose ibp_op=6 delta=[0, 0, -1, 0, 0, 1, 1] | 9 non-masters left
    Step 22: target=[3, 0, 2, 0, 1, 2, -1] w=(8,1) | 853 actions | chose ibp_op=8 delta=[0, 0, -1, 0, 0, 0, 1] | 7 non-masters left
    Step 23: target=[3, -1, 1, 0, 1, 3, 0] w=(8,1) | 886 actions | chose ibp_op=7 delta=[0, 1, 0, 0, 0, -1, 0] | 9 non-masters left
    Step 24: target=[3, -1, 1, 0, 3, 1, 0] w=(8,1) | 923 actions | chose ibp_op=5 delta=[0, 1, 0, 0, -1, 0, 0] | 7 non-masters left
    Step 25: target=[4, -1, 1, 0, 2, 1, 0] w=(8,1) | 955 actions | chose ibp_op=8 delta=[0, 1, 0, 0, -1, 0, 0] | 9 non-masters left
    Step 26: target=[4, -1, 1, 0, 1, 2, 0] w=(8,1) | 966 actions | chose ibp_op=1 delta=[0, 1, 0, 0, 0, -1, 0] | 10 non-masters left
    Step 27: target=[4, -1, 2, 0, 1, 1, 0] w=(8,1) | 1003 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 8 non-masters left
    Step 28: target=[3, 0, 2, 0, 1, 2, 0] w=(8,0) | 885 actions | chose ibp_op=0 delta=[0, -1, 0, 0, 0, -1, 0] | 1 non-masters left
    Step 29: target=[3, -2, 3, 0, 1, 1, 0] w=(8,2) | 933 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 30: target=[4, -2, 2, 0, 1, 1, 0] w=(8,2) | 970 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 31: target=[5, -2, 1, 0, 1, 1, 0] w=(8,2) | 997 actions | chose ibp_op=0 delta=[-4, 1, 3, 0, 0, 0, 0] | 1 non-masters left
    Step 32: target=[1, -2, 5, 0, 1, 1, 0] w=(8,2) | 1040 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 33: target=[2, -2, 4, 0, 1, 1, 0] w=(8,2) | 1065 actions | chose ibp_op=5 delta=[0, 1, -2, 0, 2, 0, 0] | 11 non-masters left
    Step 34: target=[2, -2, 3, 0, 3, 1, 0] w=(9,2) | 1140 actions | chose ibp_op=5 delta=[0, 1, -1, 0, -2, 2, 0] | 10 non-masters left
    Step 35: target=[2, -2, 3, 0, 1, 3, 0] w=(9,2) | 1209 actions | chose ibp_op=2 delta=[0, 1, 0, 0, 0, -2, 0] | 3 non-masters left
    Step 36: target=[2, -1, 4, -1, 1, 1, 0] w=(8,2) | 1258 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 5 non-masters left
    Step 37: target=[3, -1, 3, -1, 1, 1, 0] w=(8,2) | 1298 actions | chose ibp_op=5 delta=[0, 1, -2, 0, 0, 0, 0] | 5 non-masters left
    Step 38: target=[3, -1, 2, -1, 1, 1, 0] w=(7,2) | 1340 actions | chose ibp_op=5 delta=[0, 1, 0, 0, 0, 0, 0] | 8 non-masters left
    Step 39: target=[4, -1, 2, -1, 1, 1, 0] w=(8,2) | 1374 actions | chose ibp_op=5 delta=[0, 1, -1, 0, 0, 0, 0] | 9 non-masters left
    Step 40: target=[5, -1, 1, -1, 1, 1, 0] w=(8,2) | 1402 actions | chose ibp_op=0 delta=[-1, 1, 0, 0, 0, 0, 0] | 8 non-masters left
    Step 41: target=[3, -1, 3, 0, 1, 1, 0] w=(8,1) | 1407 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 7 non-masters left
    Step 42: target=[1, 0, 4, -1, 1, 1, 0] w=(7,1) | 1412 actions | chose ibp_op=2 delta=[0, 0, -1, 1, 0, 0, 0] | 5 non-masters left
    Step 43: target=[2, 0, 3, -1, 1, 1, 0] w=(7,1) | 1428 actions | chose ibp_op=2 delta=[0, 0, -1, 1, 0, 0, 0] | 3 non-masters left
    Step 44: target=[3, 0, 2, -1, 1, 1, 0] w=(7,1) | 1437 actions | chose ibp_op=3 delta=[0, 0, -1, 1, 0, 0, 0] | 3 non-masters left
    Step 45: target=[4, 0, 1, -1, 1, 1, 0] w=(7,1) | 1489 actions | chose ibp_op=4 delta=[-3, -2, 4, 1, 0, 0, 0] | 9 non-masters left
    Step 46: target=[1, -2, 5, 0, 2, 1, -1] w=(9,3) | 1560 actions | chose ibp_op=4 delta=[4, 0, -4, 0, -1, 0, 1] | 8 non-masters left
    Step 47: target=[5, -2, 1, 0, 2, 1, -1] w=(9,3) | 1613 actions | chose ibp_op=4 delta=[-3, 0, 3, 0, -1, 0, 1] | 10 non-masters left
    Step 48: target=[2, -2, 4, 0, 2, 1, -1] w=(9,3) | 1674 actions | chose ibp_op=4 delta=[0, 2, -2, 0, -1, 0, 1] | 13 non-masters left
    Step 49: target=[2, 0, 2, -1, 2, 1, 0] w=(7,1) | 1720 actions | chose ibp_op=3 delta=[0, 0, -1, 1, 0, 0, 0] | 12 non-masters left
  I[2,0,2,0,1,1,0]: FAILED (12 non-masters) in 50 steps
  Saved checkpoint to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/checkpoint_epoch20.pt
  Epoch 21 batch 50/692: loss=0.0347, top1=0.969
  Epoch 21 batch 100/692: loss=0.1055, top1=0.969
  Epoch 21 batch 150/692: loss=0.0220, top1=1.000
  Epoch 21 batch 200/692: loss=0.1368, top1=0.969
  Epoch 21 batch 250/692: loss=0.2044, top1=0.938
  Epoch 21 batch 300/692: loss=0.0370, top1=1.000
  Epoch 21 batch 350/692: loss=0.0091, top1=1.000
  Epoch 21 batch 400/692: loss=0.1829, top1=0.969
  Epoch 21 batch 450/692: loss=0.0860, top1=0.969
  Epoch 21 batch 500/692: loss=0.1855, top1=0.969
  Epoch 21 batch 550/692: loss=0.4984, top1=0.906
  Epoch 21 batch 600/692: loss=0.0820, top1=0.969
  Epoch 21 batch 650/692: loss=0.1908, top1=0.969
Epoch 21/30 (78.1s):
  Train: loss=0.1258, top1=0.9638, top5=0.9985
  Val:   loss=0.2467, top1=0.9430, top5=0.9951
  Epoch 22 batch 50/692: loss=0.0042, top1=1.000
  Epoch 22 batch 100/692: loss=0.2383, top1=0.938
  Epoch 22 batch 150/692: loss=0.1557, top1=0.969
  Epoch 22 batch 200/692: loss=0.1177, top1=0.938
  Epoch 22 batch 250/692: loss=0.0158, top1=1.000
  Epoch 22 batch 300/692: loss=0.6255, top1=0.938
  Epoch 22 batch 350/692: loss=0.0292, top1=1.000
  Epoch 22 batch 400/692: loss=0.0053, top1=1.000
  Epoch 22 batch 450/692: loss=0.1281, top1=0.969
  Epoch 22 batch 500/692: loss=0.0558, top1=0.969
  Epoch 22 batch 550/692: loss=0.0176, top1=1.000
  Epoch 22 batch 600/692: loss=0.1855, top1=0.906
  Epoch 22 batch 650/692: loss=0.1235, top1=0.969
Epoch 22/30 (78.1s):
  Train: loss=0.1144, top1=0.9672, top5=0.9987
  Val:   loss=0.2307, top1=0.9483, top5=0.9967
  Epoch 23 batch 50/692: loss=0.0095, top1=1.000
  Epoch 23 batch 100/692: loss=0.0393, top1=0.969
  Epoch 23 batch 150/692: loss=0.2042, top1=0.969
  Epoch 23 batch 200/692: loss=0.0183, top1=1.000
  Epoch 23 batch 250/692: loss=0.0016, top1=1.000
  Epoch 23 batch 300/692: loss=0.4358, top1=0.875
  Epoch 23 batch 350/692: loss=0.2882, top1=0.906
  Epoch 23 batch 400/692: loss=0.2558, top1=0.938
  Epoch 23 batch 450/692: loss=0.0036, top1=1.000
  Epoch 23 batch 500/692: loss=0.1044, top1=0.969
  Epoch 23 batch 550/692: loss=0.1497, top1=0.938
  Epoch 23 batch 600/692: loss=0.1383, top1=0.938
  Epoch 23 batch 650/692: loss=0.0281, top1=1.000
Epoch 23/30 (86.1s):
  Train: loss=0.1073, top1=0.9689, top5=0.9991
  Val:   loss=0.2178, top1=0.9504, top5=0.9963
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 24 batch 50/692: loss=0.1672, top1=0.969
  Epoch 24 batch 100/692: loss=0.2804, top1=0.969
  Epoch 24 batch 150/692: loss=0.0711, top1=0.969
  Epoch 24 batch 200/692: loss=0.0196, top1=1.000
  Epoch 24 batch 250/692: loss=0.1437, top1=0.938
  Epoch 24 batch 300/692: loss=0.1122, top1=0.969
  Epoch 24 batch 350/692: loss=0.2602, top1=0.969
  Epoch 24 batch 400/692: loss=0.2362, top1=0.906
  Epoch 24 batch 450/692: loss=0.0025, top1=1.000
  Epoch 24 batch 500/692: loss=0.1100, top1=0.969
  Epoch 24 batch 550/692: loss=0.0033, top1=1.000
  Epoch 24 batch 600/692: loss=0.1021, top1=0.938
  Epoch 24 batch 650/692: loss=0.0223, top1=1.000
Epoch 24/30 (77.7s):
  Train: loss=0.0997, top1=0.9711, top5=0.9991
  Val:   loss=0.2276, top1=0.9540, top5=0.9963
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 25 batch 50/692: loss=0.1201, top1=0.938
  Epoch 25 batch 100/692: loss=0.1244, top1=0.938
  Epoch 25 batch 150/692: loss=0.0612, top1=0.969
  Epoch 25 batch 200/692: loss=0.0408, top1=1.000
  Epoch 25 batch 250/692: loss=0.0113, top1=1.000
  Epoch 25 batch 300/692: loss=0.0438, top1=1.000
  Epoch 25 batch 350/692: loss=0.0242, top1=1.000
  Epoch 25 batch 400/692: loss=0.0854, top1=0.969
  Epoch 25 batch 450/692: loss=0.0037, top1=1.000
  Epoch 25 batch 500/692: loss=0.0376, top1=0.969
  Epoch 25 batch 550/692: loss=0.1170, top1=0.969
  Epoch 25 batch 600/692: loss=0.4319, top1=0.938
  Epoch 25 batch 650/692: loss=0.2707, top1=0.906
Epoch 25/30 (80.3s):
  Train: loss=0.0931, top1=0.9738, top5=0.9993
  Val:   loss=0.2257, top1=0.9528, top5=0.9967
  Epoch 26 batch 50/692: loss=0.0339, top1=1.000
  Epoch 26 batch 100/692: loss=0.0363, top1=1.000
  Epoch 26 batch 150/692: loss=0.0073, top1=1.000
  Epoch 26 batch 200/692: loss=0.0032, top1=1.000
  Epoch 26 batch 250/692: loss=0.0825, top1=0.969
  Epoch 26 batch 300/692: loss=0.4740, top1=0.938
  Epoch 26 batch 350/692: loss=0.0002, top1=1.000
  Epoch 26 batch 400/692: loss=0.0224, top1=1.000
  Epoch 26 batch 450/692: loss=0.0224, top1=1.000
  Epoch 26 batch 500/692: loss=0.0277, top1=1.000
  Epoch 26 batch 550/692: loss=0.0558, top1=0.969
  Epoch 26 batch 600/692: loss=0.0716, top1=0.969
  Epoch 26 batch 650/692: loss=0.3165, top1=0.938
Epoch 26/30 (75.6s):
  Train: loss=0.0837, top1=0.9764, top5=0.9993
  Val:   loss=0.2247, top1=0.9540, top5=0.9972
  Epoch 27 batch 50/692: loss=0.0048, top1=1.000
  Epoch 27 batch 100/692: loss=0.2247, top1=0.969
  Epoch 27 batch 150/692: loss=0.0041, top1=1.000
  Epoch 27 batch 200/692: loss=0.1122, top1=0.938
  Epoch 27 batch 250/692: loss=0.3057, top1=0.969
  Epoch 27 batch 300/692: loss=0.0061, top1=1.000
  Epoch 27 batch 350/692: loss=0.1282, top1=0.969
  Epoch 27 batch 400/692: loss=0.2224, top1=0.969
  Epoch 27 batch 450/692: loss=0.2173, top1=0.938
  Epoch 27 batch 500/692: loss=0.0113, top1=1.000
  Epoch 27 batch 550/692: loss=0.0859, top1=0.969
  Epoch 27 batch 600/692: loss=0.1762, top1=0.969
  Epoch 27 batch 650/692: loss=0.0010, top1=1.000
Epoch 27/30 (76.0s):
  Train: loss=0.0766, top1=0.9779, top5=0.9996
  Val:   loss=0.2450, top1=0.9524, top5=0.9976
  Epoch 28 batch 50/692: loss=0.0035, top1=1.000
  Epoch 28 batch 100/692: loss=0.0726, top1=0.969
  Epoch 28 batch 150/692: loss=0.0022, top1=1.000
  Epoch 28 batch 200/692: loss=0.0463, top1=0.969
  Epoch 28 batch 250/692: loss=0.3611, top1=0.938
  Epoch 28 batch 300/692: loss=0.0290, top1=1.000
  Epoch 28 batch 350/692: loss=0.0364, top1=0.969
  Epoch 28 batch 400/692: loss=0.0541, top1=0.969
  Epoch 28 batch 450/692: loss=0.0363, top1=0.969
  Epoch 28 batch 500/692: loss=0.0251, top1=1.000
  Epoch 28 batch 550/692: loss=0.0544, top1=0.969
  Epoch 28 batch 600/692: loss=0.0003, top1=1.000
  Epoch 28 batch 650/692: loss=0.3462, top1=0.906
Epoch 28/30 (76.2s):
  Train: loss=0.0778, top1=0.9798, top5=0.9994
  Val:   loss=0.2688, top1=0.9577, top5=0.9972
  -> New best val acc! Saved to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/best_model.pt
  Epoch 29 batch 50/692: loss=0.2379, top1=0.938
  Epoch 29 batch 100/692: loss=0.0024, top1=1.000
  Epoch 29 batch 150/692: loss=0.0755, top1=0.938
  Epoch 29 batch 200/692: loss=0.0648, top1=0.969
  Epoch 29 batch 250/692: loss=0.1715, top1=0.906
  Epoch 29 batch 300/692: loss=0.0017, top1=1.000
  Epoch 29 batch 350/692: loss=0.0033, top1=1.000
  Epoch 29 batch 400/692: loss=0.0141, top1=1.000
  Epoch 29 batch 450/692: loss=0.0016, top1=1.000
  Epoch 29 batch 500/692: loss=0.0602, top1=0.969
  Epoch 29 batch 550/692: loss=0.1964, top1=0.969
  Epoch 29 batch 600/692: loss=0.0738, top1=0.969
  Epoch 29 batch 650/692: loss=0.0298, top1=0.969
Epoch 29/30 (81.1s):
  Train: loss=0.0727, top1=0.9794, top5=0.9996
  Val:   loss=0.2651, top1=0.9544, top5=0.9955
  Epoch 30 batch 50/692: loss=0.1292, top1=0.969
  Epoch 30 batch 100/692: loss=0.0098, top1=1.000
  Epoch 30 batch 150/692: loss=0.2390, top1=0.906
  Epoch 30 batch 200/692: loss=0.1560, top1=0.938
  Epoch 30 batch 250/692: loss=0.0695, top1=0.969
  Epoch 30 batch 300/692: loss=0.0071, top1=1.000
  Epoch 30 batch 350/692: loss=0.1783, top1=0.969
  Epoch 30 batch 400/692: loss=0.0138, top1=1.000
  Epoch 30 batch 450/692: loss=0.0112, top1=1.000
  Epoch 30 batch 500/692: loss=0.0002, top1=1.000
  Epoch 30 batch 550/692: loss=0.0010, top1=1.000
  Epoch 30 batch 600/692: loss=0.0083, top1=1.000
  Epoch 30 batch 650/692: loss=0.0014, top1=1.000
Epoch 30/30 (77.2s):
  Train: loss=0.0684, top1=0.9818, top5=0.9995
  Val:   loss=0.2822, top1=0.9544, top5=0.9967
  Testing on I[2,0,2,0,1,1,0]...
    Step 0: target=[2, 0, 2, 0, 1, 1, 0] w=(6,0) | 52 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 0, 0, 0] | 8 non-masters left
    Step 1: target=[2, 0, 2, 0, 1, 1, -1] w=(6,1) | 100 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 1, 0, 1] | 10 non-masters left
    Step 2: target=[2, 0, 2, 0, 2, 1, -1] w=(7,1) | 154 actions | chose ibp_op=3 delta=[0, 0, -1, 0, -1, 1, 1] | 9 non-masters left
    Step 3: target=[2, 0, 2, -1, 1, 2, 0] w=(7,1) | 207 actions | chose ibp_op=0 delta=[0, 0, 0, 1, 0, -1, 0] | 2 non-masters left
    Step 4: target=[2, -1, 3, 0, 1, 1, 0] w=(7,1) | 256 actions | chose ibp_op=5 delta=[-1, 1, 0, 0, 0, 0, 0] | 2 non-masters left
    Step 5: target=[1, -1, 4, 0, 1, 1, 0] w=(7,1) | 302 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 6: target=[1, 0, 3, 0, 1, 1, 0] w=(6,0) | 322 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 0, 0, 0] | 8 non-masters left
    Step 7: target=[1, 0, 3, 0, 1, 1, -1] w=(6,1) | 357 actions | chose ibp_op=7 delta=[0, 0, -1, 0, 1, 0, 1] | 10 non-masters left
    Step 8: target=[1, 0, 3, 0, 2, 1, -1] w=(7,1) | 398 actions | chose ibp_op=7 delta=[0, 0, -1, 0, -1, 1, 1] | 10 non-masters left
    Step 9: target=[1, 0, 3, 0, 1, 2, -1] w=(7,1) | 439 actions | chose ibp_op=5 delta=[1, 0, -1, 0, 0, -1, 1] | 2 non-masters left
    Step 10: target=[3, -1, 2, 0, 1, 1, 0] w=(7,1) | 482 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 1 non-masters left
    Step 11: target=[3, 0, 1, 0, 1, 1, 0] w=(6,0) | 496 actions | chose ibp_op=7 delta=[0, 0, 0, 0, 0, 0, 0] | 7 non-masters left
    Step 12: target=[3, 0, 2, 0, 1, 1, -1] w=(7,1) | 529 actions | chose ibp_op=1 delta=[0, 0, -1, 0, 0, 0, 1] | 4 non-masters left
    Step 13: target=[3, -1, 1, 0, 1, 2, 0] w=(7,1) | 571 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 2 non-masters left
    Step 14: target=[3, -1, 1, 0, 2, 1, 0] w=(7,1) | 607 actions | chose ibp_op=5 delta=[0, 1, 0, 0, -1, 0, 0] | 1 non-masters left
    Step 15: target=[4, -1, 1, 0, 1, 1, 0] w=(7,1) | 632 actions | chose ibp_op=3 delta=[-1, 1, 0, 0, 0, 0, 0] | 2 non-masters left
    Step 16: target=[3, 0, 2, -1, 1, 1, 0] w=(7,1) | 660 actions | chose ibp_op=2 delta=[0, 0, -1, 1, 0, 0, 0] | 3 non-masters left
    Step 17: target=[4, 0, 1, -1, 1, 1, 0] w=(7,1) | 688 actions | chose ibp_op=4 delta=[-3, 0, 2, 1, 0, 0, 0] | 12 non-masters left
    Step 18: target=[1, 0, 3, -1, 1, 2, 0] w=(7,1) | 722 actions | chose ibp_op=1 delta=[0, 0, 0, 1, 0, -1, 0] | 4 non-masters left
    Step 19: target=[1, -1, 3, 0, 1, 2, 0] w=(7,1) | 773 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 2 non-masters left
    Step 20: target=[1, -1, 3, 0, 2, 1, 0] w=(7,1) | 819 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 21: target=[1, 0, 3, 0, 1, 2, 0] w=(7,0) | 810 actions | chose ibp_op=5 delta=[0, 0, -1, 0, 0, 0, 0] | 4 non-masters left
    Step 22: target=[2, -1, 2, 0, 1, 2, 0] w=(7,1) | 853 actions | chose ibp_op=0 delta=[-1, 1, 0, 0, 0, 0, 0] | 2 non-masters left
    Step 23: target=[1, 0, 3, 0, 2, 1, 0] w=(7,0) | 798 actions | chose ibp_op=0 delta=[0, 0, 0, 0, 0, 0, 0] | 2 non-masters left
    Step 24: target=[1, -1, 4, 0, 2, 1, 0] w=(8,1) | 850 actions | chose ibp_op=8 delta=[0, 1, 0, 0, -1, 0, 0] | 4 non-masters left
    Step 25: target=[1, -1, 4, 0, 1, 2, 0] w=(8,1) | 886 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 4 non-masters left
    Step 26: target=[1, 0, 4, 0, 1, 2, 0] w=(8,0) | 931 actions | chose ibp_op=3 delta=[0, 0, -1, 0, 0, -1, 0] | 2 non-masters left
    Step 27: target=[1, 0, 4, -1, 1, 1, 0] w=(7,1) | 968 actions | chose ibp_op=0 delta=[0, 0, -1, 0, 0, 0, 0] | 2 non-masters left
    Step 28: target=[1, -1, 4, -1, 1, 1, 0] w=(7,2) | 1004 actions | chose ibp_op=0 delta=[3, 1, -3, 0, 0, 0, 0] | 3 non-masters left
    Step 29: target=[4, -1, 2, -1, 1, 1, 0] w=(8,2) | 1044 actions | chose ibp_op=0 delta=[-1, 1, 0, 0, 0, 0, 0] | 4 non-masters left
    Step 30: target=[3, -1, 3, -1, 1, 1, 0] w=(8,2) | 1078 actions | chose ibp_op=0 delta=[-1, 1, 0, 0, 0, 0, 0] | 4 non-masters left
    Step 31: target=[2, -1, 4, -1, 1, 1, 0] w=(8,2) | 1109 actions | chose ibp_op=0 delta=[0, 1, -2, 0, 0, 0, 0] | 4 non-masters left
    Step 32: target=[2, -1, 3, -1, 1, 1, 0] w=(7,2) | 1131 actions | chose ibp_op=0 delta=[0, 1, -1, 1, 1, 0, 0] | 3 non-masters left
    Step 33: target=[2, -1, 3, 0, 2, 1, 0] w=(8,1) | 1173 actions | chose ibp_op=8 delta=[0, 1, 0, 0, -1, 0, 0] | 5 non-masters left
    Step 34: target=[2, -1, 3, 0, 1, 2, 0] w=(8,1) | 1204 actions | chose ibp_op=1 delta=[0, 1, 0, 0, 0, -1, 0] | 6 non-masters left
    Step 35: target=[2, -1, 4, 0, 1, 1, 0] w=(8,1) | 1238 actions | chose ibp_op=0 delta=[0, 1, -1, 0, 0, 0, 0] | 4 non-masters left
    Step 36: target=[2, 0, 3, 0, 1, 2, 0] w=(8,0) | 1205 actions | chose ibp_op=0 delta=[0, 0, -1, 0, 0, 0, 0] | 4 non-masters left
    Step 37: target=[2, 0, 3, 0, 2, 1, 0] w=(8,0) | 1265 actions | chose ibp_op=0 delta=[0, 0, 0, 0, 0, 0, 0] | 5 non-masters left
    Step 38: target=[2, -1, 4, 0, 2, 1, 0] w=(9,1) | 1317 actions | chose ibp_op=1 delta=[0, 1, 0, 0, -1, 0, 0] | 10 non-masters left
    Step 39: target=[2, -1, 4, 0, 1, 2, 0] w=(9,1) | 1346 actions | chose ibp_op=1 delta=[0, 1, -2, 0, 0, -1, 0] | 5 non-masters left
    Step 40: target=[2, -1, 2, 0, 2, 1, 0] w=(7,1) | 1399 actions | chose ibp_op=8 delta=[0, 1, 0, 0, -1, 0, 0] | 3 non-masters left
    Step 41: target=[2, 0, 2, 0, 2, 1, 0] w=(7,0) | 1433 actions | chose ibp_op=5 delta=[0, 0, -1, 0, 0, 0, 0] | 4 non-masters left
    Step 42: target=[2, 0, 2, 0, 1, 2, 0] w=(7,0) | 1460 actions | chose ibp_op=8 delta=[0, 0, 0, 0, 0, 0, 0] | 9 non-masters left
    Step 43: target=[2, -1, 2, 0, 1, 3, 0] w=(8,1) | 1517 actions | chose ibp_op=1 delta=[0, 1, 0, 0, 0, -1, 0] | 7 non-masters left
    Step 44: target=[2, -1, 2, 0, 2, 2, 0] w=(8,1) | 1561 actions | chose ibp_op=8 delta=[0, 1, 0, 0, 0, -1, 0] | 9 non-masters left
    Step 45: target=[2, -1, 2, 0, 3, 1, 0] w=(8,1) | 1610 actions | chose ibp_op=1 delta=[0, 1, 0, 0, -1, 0, 0] | 7 non-masters left
    Step 46: target=[2, 0, 2, 0, 1, 3, 0] w=(8,0) | 1651 actions | chose ibp_op=5 delta=[0, 0, -1, 0, 0, -1, 0] | 2 non-masters left
    Step 47: target=[3, 0, 1, 0, 2, 1, 0] w=(7,0) | 1641 actions | chose ibp_op=5 delta=[-1, 0, 1, 0, 0, 0, 0] | 3 non-masters left
    Step 48: target=[3, -1, 2, 0, 2, 1, 0] w=(8,1) | 1668 actions | chose ibp_op=8 delta=[0, 1, 0, 0, -1, 0, 0] | 5 non-masters left
    Step 49: target=[3, -1, 2, 0, 1, 2, 0] w=(8,1) | 1694 actions | chose ibp_op=1 delta=[0, 1, 0, 0, 0, -1, 0] | 6 non-masters left
  I[2,0,2,0,1,1,0]: FAILED (6 non-masters) in 50 steps
  Saved checkpoint to /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/checkpoint_epoch30.pt

======================================================================
Training complete!
Best validation accuracy: 0.9577
Final model saved to: /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/final_model.pt
Training history saved to: /home/shih/work/IBPreduction/checkpoints/classifier_v3_filtered/training_history.json
